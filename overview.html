<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Overview | EARL: Environments for Autonomous Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link rel="stylesheet" href="vendor/simple-line-icons/css/simple-line-icons.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css">

  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400;500;600;700&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css2?family=Zilla+Slab:wght@300;400&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli" rel="stylesheet">

  <!-- Plugin CSS -->
  <link rel="stylesheet" href="device-mockups/device-mockups.min.css">

  <!-- Custom styles for this template -->
  <link href="css/new-age.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

</head>

<body id="page-top">

<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="index.html">EARL</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="overview.html" style="color:rgba(0,0,0,0.3) !important">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="environments.html">Environments</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://arxiv.org/abs/TBD">Paper</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://github.com/architsharma97/earl_benchmark"">Code</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  <section class="features" id="alltasks">
    <div class="container">
      <h2>Overview</h2>
      <div class="row">
        <div class="col-lg">
          <figure>
            <img src="img/rl.svg" class="img-fluid" alt="">
            <figcaption class="text-center">Typical episodic reinforcement learning training.</figcaption>
          </figure>
        </div>
        <div class="col-lg">
            <figure>
              <img src="img/arl.svg" class="img-fluid" alt="">
              <figcaption class="text-center">Autonomous reinforcement learning training.</figcaption>
            </figure>
        </div>
      </div>
      <br>
      <p>
        Embodied agents such as humans and robots are situated in a continual non-episodic world. Reinforcement learning (RL) promises a framework for enabling artificial agents to learn autonomously with minimal human intervention. However, current episodic RL routine takes the following form:
      <p>
      <ol>
        <li>Sample an initial state <i>s</i>.</li>
        <li>Let the agent run from <i>s</i> for a short period of time (typically 100 to 1000 steps).</li>
        <li>Update the agent (policy, model etc).</li>
        <li>Repeat till convergence.</li>
      </ol>
      <!--Most reinforcement learning benchmarks focus on providing environments with interesting dynamics and state spaces. 
      Researchers tend to follow the above procedure to measure the performance of their algorithms.-->
      <p> This algorithmic routine relies on the existence of episodes, an assumption that breaks the autonomy of the learning system and cannot be realized without extrinsic interventions to reset the environment after every interaction.</p>

<!--       <p>But this idealized setting doesn't always line up with real-world reinforcement learning. In some cases, resets may have a high cost,
      such as time or labor, or may not be possible at all. How do you know if your learning algorithm will perform well in these scenarios?</p> -->

      <p> <b> Environment for Autonomous RL </b> (EARL): The goal of our proposed framework Autonomous RL (ARL) and the accompanying benchmark EARL is to encourage research that develops algorithms for the continual non-episodic world, moving towards building truly autonomous embodied agents. At a high level, algorithms are evaluated on EARL under the following conditions:</p>
      <ol>
        <li>Sample an initial state <i>s</i>.</li>
        <li>With probability 1-&epsilon;, the agent runs from <i>s</i> according to the environment dynamics.</li>
        <li>With probability &epsilon;, an extrinsic intervention resets the environment to a newly sampled initial state.</li>
      </ol>
      <p>In the EARL environments, &epsilon; is very low (10<sup>-5</sup> to 10<sup>-6</sup>), such that the agents operate autonomously for several hundred thousands of steps. EARL provides a diverse set of environments to evaluate autonomous learning algorithms, offering two modes for evaluating the algorithms:</p>

      <ul>
        <li><strong>Deployment Evaluation:</strong> The learned policy is intermittently evaluated from the initial state, representing the performance of the policy if it were deployed.</li>
        <li><strong>Continuing Evaluation:</strong> Algorithms are evaluated on the rate at which they accumulate the reward during their lifetimes.</li>
      </ul>
      
      <img src="img/eval.png" class="img-fluid" style="width:100%; max-width: 50em; display:block; margin-left: auto; margin-right: auto">
      <br>
      <p>
      As motivation, consider training a robot to clean a kitchen. We might want to evaluate how well the robot keeps the kitchen clean over it's entire lifetime, as well as how it performs if it is deployed for a specific cleaning task, like turning off the stove. Both objectives are important, but may not always align, so EARL returns both values for all environments.
        </p>
      <p>
        The EARL benchmark consists of 6 simulated environments, ranging from the aforementioned kitchen environment to dextrous hand manipulation and locomotive robots.
        Learn more about them on <a href="environments.html">our environments page &rarr;</a>
    </p>
    </div>
</section>

<section class="features" style="background: rgba(253,187,45,0.075);">
  <div class="container">
    <h2>Get Started</h2>
      <p>First, clone the repository and place it in in your PYTHONPATH:</p>
      <pre>
        <code class="language-bash">
      git clone https://github.com/architsharma97/earl_benchmark.git
        </code>
      </pre>
      <p>To import the environments, run:</p>
      <pre>
        <code class="language-python">
      import earl_benchmark

      # Options are tabletop_manipulation, sawyer_door, sawyer_peg, kitchen, minitaur
      env_loader = earl_benchmark.EARLEnvs('tabletop_manipulation')
      train_env, eval_env = env_loader.get_envs()
      initial_states = env_loader.get_initial_states()
      goal_states = env_loader.get_goal_states()
        </code>
      </pre>
  </div>
</section>

  <section class="features" id="about">
    <div class="container">
      <div class="section-heading ">
           <h2>Authors</h2>
           <ul>
            <li><a href="https://architsharma97.github.io/">Archit Sharma</a>, Stanford University</li>
            <li><a href="https://kelvinxu.github.io/">Kelvin Xu</a>, UC Berkeley</li>
            <li><a href="https://nikhilsardana.github.io/">Nikhil Sardana</a>, Stanford University</li>
            <li><a href="https://scholar.google.com/citations?user=1wLVDP4AAAAJ&hl=en">Abhishek Gupta</a>, UC Berkeley</li>
            <li><a href="https://karolhausman.github.io/">Karol Hausman</a>, Google &amp; Stanford University</li>
            <li><a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, UC Berkeley</li>
            <li><a href="https://cs.stanford.edu/~cbfinn/">Chelsea Finn</a>, Stanford University</li>
           </ul>
           <p>If you use this benchmark, please cite our paper:</p>
        
        <pre>
          <code class="language-latex">
  @article{sharma2021Autonomous,
    title={Autonomous Reinforcement Learning: Benchmarking and Formalism},
    author={Archit Sharma and Kelvin Xu and Nikhil Sardana and Abhishek Gupta and Karol Hausman and Sergey Levine and Chelsea Finn},
    journal={arXiv preprint arXiv:21TBD},
    year={2021}
  }
          </code>
        </pre>
      </div>
      </div>
    </div>
  </section>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/new-age.min.js"></script>

</body>

</html>
