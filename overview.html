<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Overview | EARL: Environments for Autonomous Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link rel="stylesheet" href="vendor/simple-line-icons/css/simple-line-icons.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css">

  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400;500;600;700&display=swap" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css2?family=Zilla+Slab:wght@300;400&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli" rel="stylesheet">

  <!-- Plugin CSS -->
  <link rel="stylesheet" href="device-mockups/device-mockups.min.css">

  <!-- Custom styles for this template -->
  <link href="css/new-age.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

</head>

<body id="page-top">

<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="index.html">EARL</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="overview.html" style="color:rgba(0,0,0,0.3) !important">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="environments.html">Environments</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://arxiv.org/abs/TBD">Paper</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://github.com/architsharma97/persistent_rl_benchmark"">Code</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  <section class="features" id="alltasks">
    <div class="container">
      <h2>Overview</h2>
      <div class="row">
        <div class="col-lg">
          <figure>
            <img src="img/rl.svg" class="img-fluid" alt="">
            <figcaption class="text-center">Typical episodic reinforcement learning training.</figcaption>
          </figure>
        </div>
        <div class="col-lg">
            <figure>
              <img src="img/arl.svg" class="img-fluid" alt="">
              <figcaption class="text-center">Autonomous reinforcement learning training.</figcaption>
            </figure>
        </div>
      </div>
      <br>
      <p>
        Typically, reinforcement learning consists of training an agent by repeatedly running it in an episodic environment. 
        The learning procedure often takes the following form:
      <p>
      <ol>
        <li>Sample an initial state <i>s</i>.</li>
        <li>Let the agent run from <i>s</i> for a short period of time (typically 10<sup>2</sup> to 10<sup>3</sup> steps).</li>
        <li>Update the learning algorithm and repeat from step (1) until convergence.</li>
        <li>Report the final performance.</li>
      </ol>
      <!--Most reinforcement learning benchmarks focus on providing environments with interesting dynamics and state spaces. 
      Researchers tend to follow the above procedure to measure the performance of their algorithms.-->      
      
      <p>But this idealized setting doesn't always line up with real-world reinforcement learning. In some cases, resets may have a high cost,
      such as time or labor, or may not be possible at all. How do you know if your learning algorithm will perform well in these scenarios?</p>

      <p>Enter EARL: A collection of environments designed to test how your algorithm fares in complex settings with limited resets. 
      We provide a formal definition of <i>autonomous reinforcement learning</i> in our paper, but in short, when you run your algorithm on an EARL environment, the training procedure is as follows:</p>
      <ol>
        <li>Sample an initial state <i>s</i>.</li>
        <li>With probability 1-&epsilon;, your agent runs from <i>s</i> according to the environment dynamics.</li>
        <li>With probability &epsilon;, an intervention occurs and your agent resets to a newly sampled initial state.</li>
      </ol>
      <p>In the EARL environments, &epsilon; is set low (10<sup>-5</sup> to 10<sup>-6</sup>) to mimic a high reset cost.</p>

      <p>EARL provides two different envaluation modes, representing two different ways autonomous RL can be used in the real-world:</p>

      <ul>
        <li><strong>Deployment Evaluation:</strong> Algorithms first train according to the procedure above. Afterwards, algorithms are evaluated on the environment with standard-length episodes.</li>
        <li><strong>Continuing Evaluation:</strong> Algorithms are evaluated on their cumulative performance from the initial state.</li>
      </ul>
      
      <img src="img/eval.png" class="img-fluid" style="width:100%; max-width: 50em; display:block; margin-left: auto; margin-right: auto">
      <br>
      <p>
      As motivation, consider training a robot to clean a kitchen. We might want to evaluate how well the robot keeps the kitchen clean over it's entire lifetime, as well as how it performs if it is deployed for a specific cleaning task, like turning off the stove. Both objectives are important, but may not always align, so EARL returns both values for all environments.
        </p>
      <p>
        The EARL benchmark consists of 5 simulated environments, ranging from the aforementioned kitchen environment to dextrous hand manipulation and locomotive robots.
        Learn more about them on <a href="environments.html">our environments page &rarr;</a>
    </p>
    </div>
</section>

<section class="features" style="background: rgba(253,187,45,0.075);">
  <div class="container">
    <h2>Get Started</h2>
      <p>First, clone the repository and place it in in your PYTHONPATH:</p>
      <pre>
        <code class="language-bash">
      git clone https://github.com/architsharma97/earl_benchmark.git
        </code>
      </pre>
      <p>To import the environments, run:</p>
      <pre>
        <code class="language-python">
      import earl_benchmark

      # Options are tabletop_manipulation, sawyer_door, sawyer_peg, kitchen, minitaur
      env_loader = earl_benchmark.EARLEnvs('tabletop_manipulation')
      train_env, eval_env = env_loader.get_envs()
      initial_states = env_loader.get_initial_states()
      goal_states = env_loader.get_goal_states()
        </code>
      </pre>
  </div>
</section>

  <section class="features" id="about">
    <div class="container">
      <div class="section-heading ">
           <h2>Authors</h2>
           <ul>
            <li><a href="https://cs.stanford.edu/~arcitsh/">Archit Sharma</a>, Stanford University</li>
            <li><a href="https://kelvinxu.github.io/">Kelvin Xu</a>, UC Berkeley</li>
            <li><a href="https://nikhilsardana.github.io/">Nikhil Sardana</a>, Stanford University</li>
            <li><a href="https://scholar.google.com/citations?user=1wLVDP4AAAAJ&hl=en">Abhishek Gupta</a>, UC Berkeley</li>
            <li><a href="https://karolhausman.github.io/">Karol Hausman</a>, Google &amp; Stanford University</li>
            <li><a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, UC Berkeley</li>
            <li><a href="https://cs.stanford.edu/~cbfinn/">Chelsea Finn</a>, Stanford University</li>
           </ul>
           <p>If you use this benchmark, please cite our paper:</p>
        
        <pre>
          <code class="language-latex">
  @article{sharma2021Autonomous,
    title={Autonomous Reinforcement Learning: Benchmarking and Formalism},
    author={Archit Sharma and Kelvin Xu and Nikhil Sardana and Abhishek Gupta and Karol Hausman and Sergey Levine and Chelsea Finn},
    journal={arXiv preprint arXiv:21TBD},
    year={2021}
  }
          </code>
        </pre>
      </div>
      </div>
    </div>
  </section>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/new-age.min.js"></script>

</body>

</html>
